\sect{System Architecture}
\label{subsec:system-architecture}

\subsect{Tree Parameters}
\label{subsec:tree-parameters}

\subsubsect{Relations of Parameters}

In order to reap the full benefit of the tree data structure, an informed choice
of parameters must be made that takes consideration of the expected workload and
usage of scarce memory resources.

The tree's branching factor $m$ defines the maximum number of children that each
node can have. A level $k$ down the tree from the root will contain $m^k$ nodes.
Thus, a tree of height $h$ will contain
$\sum_{k=1}^h m^k = \frac{1-m^h}{1-m}$ nodes.

The ratio of leaf nodes to internal nodes is given by:
$$
	\frac{m^{h-1}}{\pfrac{1-m^{h-1}}{1-m}}
	= \frac{m^{h-1}(1-m)}{1-m^{h-1}}
	= \frac{m^{h-1}-m^{h}}{1-m^{h-1}}
	% = \frac{m^h-m^{h+1}}{m-m^h}
	% = \frac{m^h(1-m)}{m^h(m^{1-h} - 1)}
	= \frac{1-m}{m^{1-h} - 1}
$$

As $h$ increases, this rapidly converges to $m-1$ as shown in
\autoref{fig:inner-to-height}.

\begin{figure}
	\centering
	\input{pgfplots/inner-to-height}
	\caption{Leaf to Inner Node Ratio vs Height}
	\label{fig:inner-to-height}
\end{figure}

As the height of a tree grows, the portion of overall memory that is used for
leaves decreases, converging to $1-\frac{1}{m}$ as shown in
\autoref{fig:memory-to-height}. Higher branching factors spend less memory on
traversal data. Recall that internal nodes do not store data; they store
pointers to other nodes.

\begin{figure}
	\centering
	\input{pgfplots/memory-to-height}
	\caption{Leaves as Percentage of Total Memory vs Height}
	\label{fig:memory-to-height}
\end{figure}

A tree containing $N$ nodes will have the height shown below.

\begin{align*}
	N &= \frac{1-m^h}{1-m} \\
	N (1-m) &= 1-m^h \\
	N (1-m) - 1 &= -m^h \\
	1 - N (1-m) &= m^h \\
	\log_m\left(1 - N (1-m)\right) &= h
\end{align*}

As $N$ grows, this relation can be approximated as $h \approx 1 + \log_m(N)$,
which is equivalent to  $m^{h-1} \approx N$. Because $m^{h-1}$ is the number of
leaf nodes, this expression approximates the total count of nodes as the number
of leaf nodes. As follows from previous derivations and shown in
\autoref{fig:nodes-to-height}, this approximation is more accurate for larger $m$
where there are more leaves per inner node.

\begin{figure}
	\centering
	\input{pgfplots/nodes-to-height}
	\caption{Height vs Node Count}
	\label{fig:nodes-to-height}
\end{figure}


\subsubsect{Choosing Parameters}

The optimal choice of tree parameters will likely be different between FPGA and
CPU implementations due to optimizations that HLS can make to source code that
are infeasible or impossible on a CPU.

When checking all of a node's keys in order to find a specific key, a CPU must
evaluate each key one at a time. The can be accelerated with binary search so
that it can be performed in $O(\log_2 m)$. However, on an FPGA all keys can be
evaluated \emph{simultaneously}, i.e. $O(1)$, and in fact this is Vitis' default
behavior when translating \texttt{for} loops with a sufficiently low, constant
iteration count into hardware.


\subsect{Concurrency}
\label{subsec:concurrency}

Our design is based on that of \citeauthor{base} in \citetitle{base}. Their
design uses a CPU cluster with an RDMA interconnect. The design follows the
Network-Attached-Memory (NAM) architecture, where some nodes are dedicated to
computation while others are dedicated to storage
\autocite{base,binnig-vldb-2016}.

To support concurrent access, their design uses optimistic lock coupling rather
than traditional lock coupling. This strategy does not protect areas from
concurrent access, but simply identifies when data has been changed. If two
writers begin modifying the same data, the one who writes back to main memory
second will see that its version is not what was expected, and attempt to
restart its operation using this new data. A key advantage of this strategy is
reducing cache misses due to cache invalidations that result when frequently
writing to lock bits in main memory on multi-core CPUs
\autocite{leis-damon-2016}. On an FPGA, we have the freedom to design our own
caching protocol rather than mimic the general purpose caching protocols that
are fixed in the silicon of CPUs. Thus, this problem could be negated by
managing lock bits differently from other memory to reduce caching overhead
rather than avoiding writes altogether. As noted by
\citeauthor{binnig-vldb-2016}, DBMSs function best with full control over memory
management \autocite{binnig-vldb-2016}.


\subsect{Memory Layout}
\label{subsec:memory-layout}

Though FPGAs still have a memory hierarchy as shown in
\autoref{fig:memory-hierarchy}, it functions differently to that of a CPU.

\begin{figure}
	\input{tikz/memory-heirarchy}
	\caption{Memory Hierarchy Comparison}
	\label{fig:memory-heirarchy}
\end{figure}

Even without the considerations of caching that would arise on CPU-based
systems, memory access patterns can still have significant performance impacts.
The XCU280 FPGA integrates $\SI{8}{\giga\byte}$ of on-chip High-Bandwidth Memory
(HBM) with a theoretical maximum bandwidth of $\SI{460}{\giga\byte\per\second}$
\autocite{u280}, but this is contingent on spreading out the accesses across all
available channels \autocite{holzinger-ipdpsw-2021}.

Though using HLS prevents some of the lower-level access optimizations proposed
by \citeauthor{holzinger-ipdpsw-2021}, it is still possible to reap some
benefits by controlling memory access patterns. Because the manner that the
abstract tree is accessed is dictated by \citeauthor{b-link}'s original
algorithm, the best way to optimize memory accesses is to consider how best to
lay out the tree in memory.

\begin{figure}
	\centering
	\input{tikz/grid-layout}
	\caption{Rectangular Grid Layout}
	\label{fig:grid-layout}
\end{figure}

Two memory layout schemes have been implemented. The first maps the tree to a
simple rectangular matrix as shown in \autoref{fig:grid-layout}. Each row of the
matrix corresponds to nodes at the same same level of the tree. Note that these
are not all inherently siblings of one another, as leaf nodes with different
parents are still at the leaf-node level of the tree. This strategy maintains
locality for tree levels and allows for easy determination of what level a node
resides in, but wastes more memory the closer a level is to the root.

This strategy also imposes a stricter height limit on the tree than may be
possible with a given amount of memory. Though B-trees are considered
``self-balancing'' in the academic sense due to their limits on underfull nodes,
there is no guarantee of a constant height for all sub-trees. The easiest
demonstration of this is to observe the insertion of a sequence of monotonically
increasing keys shown in \autoref{fig:sequential-imbalance}.

\begin{figure}
	\centering
	\input{tikz/sequential-imbalance}
	\caption{Inner Node Structure for Sequential Insertions on an $m=4$ Tree}
	\label{fig:sequential-imbalance}
\end{figure}


\subsect{FPGA Implementation}
\label{subsec:fpga-implementation}

\begin{figure}
	\centering
	\input{tikz/hls-arch}
	\caption{Architecture of HLS Kernel}
	\label{fig:hls-arch}
\end{figure}

The FPGA's HLS implementation wraps a C implementation of the B-Link tree that
uses purely statically allocated memory. I/O between the host CPU system and the
FPGA is performed over PCIe using shared memory buffers. Separate buffers are
used for instructions and data memory, as this both simplifies access and gives
Vitis more flexibility to parallelize access to these buffers in order to
maximize memory bandwidth utility.

The compute kernel itself consists of four modules as shown in
\autoref{fig:hls-arch}; two handle I/O encoding/decoding and two handle tree
operations. Requests and responses are similar in structure to CPU operations; a
short opcode at the front delineates the operation and the remaining bits are
instruction specific, though care has been taken to align identical fields
shared between instructions so that they appear at the same offset. Operations
other than \texttt{NOP} return a status code indicating whether the operation
succeeded and the cause of its failure if applicable.

Though each search \& insert module can only handle one request at a time,
multiple copies of the kernel can be instantiated on the same FPGA, each of
which share the same host memory but with separate request/response buffers,
allowing for an analog to CPU multithreading.
